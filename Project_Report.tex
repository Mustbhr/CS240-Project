\documentclass[10pt,twocolumn]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{parskip}
\usepackage{booktabs}
\usepackage{caption}
\geometry{margin=1in}

\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}

\title{\textbf{CS240 Project Report — Fall 2025}}
\author{Author(s): Mustafa Albahrani, Mohammed Alkhalifah \\ Email(s): mustafa.albahrani@kaust.edu.sa, mohammed.alkhalifah@kaust.edu.sa \\ Type: Reproduction Study \\ Target Traits: Reliability / Scalability }
\date{}

\begin{document}

\maketitle

\section{Context and Motivation}

Modern large-scale AI training runs across thousands of GPUs for days or weeks, making hardware and network failures inevitable. Traditional checkpointing to slow persistent storage creates large recovery delays and wasted GPU-hours. When a failure occurs, all workers must pause, reload the latest checkpoint from disk (often via NFS), and resume from that point—a process that can take several minutes and waste significant computational resources.

The \textbf{Gemini system} (SOSP 2023)~\cite{gemini2023} proposes an elegant solution: using aggregated CPU memory as a fast, distributed checkpoint tier. By storing checkpoints in RAM and replicating them across training nodes, Gemini achieves near-instant recovery while maintaining training throughput.

\subsection{Reproduction Goal}

This project aims to reproduce Gemini's key result: \textbf{$>$13$\times$ faster failure recovery with minimal overhead}, and to demonstrate the feasibility of in-memory checkpointing in a scaled-down academic cluster environment. We compare recovery latency and throughput against a traditional disk-based checkpointing baseline.

\section{Design and Architecture}

Our implementation follows the core Gemini architecture with adaptations for our cloud GPU environment.

\subsection{System Components}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Baseline Trainer}: Traditional distributed training using PyTorch's DistributedDataParallel (DDP) with periodic disk-based checkpointing for comparison.
    
    \item \textbf{In-Memory Checkpoint Manager}: The core innovation—stores model state, optimizer state, and training metadata in CPU RAM instead of disk.
    
    \item \textbf{Replication Manager}: Implements point to point replication where checkpoint are shared to next GPU i.e. GPU 1 $\rightarrow$ GPU2 $\rightarrow$ GPU3 $\rightarrow$ GPU4 $\rightarrow$ GPU1.
\end{enumerate}

\subsection{Checkpoint Flow}

During training, checkpoints are captured at configurable intervals:
\begin{enumerate}
    \item Model and optimizer states are serialized to CPU RAM (avoiding GPU memory pressure)
    \item Checkpoint shards are asynchronously replicated to peer nodes
    \item On failure, the failed worker recovers from a peer's RAM copy instead of disk
\end{enumerate}

\subsection{Technologies}

\begin{itemize}[leftmargin=*]
    \item \textbf{Framework}: PyTorch 2.0+ with DDP
    \item \textbf{Communication}: NCCL for gradients, TCP for checkpoint transfer
    \item \textbf{Hardware}: 4$\times$ NVIDIA A100 GPUs (Vast.ai)
    \item \textbf{Model}: 12-layer Transformer (1024 hidden size, 16 attention heads)
\end{itemize}

\section{Implementation Details}

\subsection{Development Environment}

We developed and tested on Vast.ai cloud infrastructure using 4$\times$ NVIDIA A100 GPUs, Python 3.9+, PyTorch 2.8, and CUDA 12.9. The codebase is organized into modular components: \texttt{src/checkpointing/} for checkpoint logic, \texttt{scripts/} for main runs and experimentation, and \texttt{src/training/} for the training loop configuration setups for baseline vs gemini.

\subsection{Key Implementation Decisions}

\textbf{Serialization Strategy}: We use \texttt{torch.save()} with \texttt{io.BytesIO} buffers to serialize checkpoints directly to RAM, avoiding disk I/O entirely during the save path.

\textbf{Memory Management}: Checkpoints are stored in pinned CPU memory to enable fast GPU-to-CPU transfers. We maintain a rolling buffer of the two most recent checkpoints to enable recovery while a new checkpoint is being created.

\textbf{Simulated Failures}: Due to cloud infrastructure constraints, we implemented failure injection at the application level rather than killing actual processes, allowing controlled measurement of recovery latency.

\subsection{Challenges Encountered}

\begin{itemize}[leftmargin=*]
    \item \textbf{Cloud GPU Access}: Limited GPU-hours required careful experiment planning and use of synthetic datasets for initial development.
    \item \textbf{Network Variability}: Inter-node bandwidth varied, affecting replication times. We report mean and standard deviation across runs.
    \item \textbf{Memory Constraints}: Large model checkpoints ($\sim$2.5 GB each) required careful memory management to avoid OOM errors.
\end{itemize}

\section{Evaluation}

\subsection{Experimental Setup}

\begin{itemize}[leftmargin=*]
    \item \textbf{Model}: 12-layer Transformer (1024 hidden, 16 heads, $\sim$630M parameters)
    \item \textbf{Iterations}: 100 training iterations per run
    \item \textbf{Checkpoint Frequency}: Every 25 iterations
    \item \textbf{Number of Runs}: 3 runs, aggregated with mean and standard deviation
    \item \textbf{Configurations}: Single-GPU (baseline vs Gemini) and Multi-GPU (4 GPUs)
\end{itemize}

\subsection{Results}

\begin{table}[h]
\centering
\caption{Single-GPU Performance Comparison}
\label{tab:single-gpu}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Metric} & \textbf{Disk} & \textbf{RAM} & \textbf{Speedup} \\
\midrule
Checkpoint (ms) & 7012$\pm$350 & 512$\pm$3 & \textbf{13.7$\times$} \\
Recovery (ms) & 820$\pm$110 & 134$\pm$57 & \textbf{6.1$\times$} \\
Throughput (it/s) & 20.1$\pm$0.6 & 40.3$\pm$0.1 & \textbf{+100\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Multi-GPU (4 GPUs) Performance Comparison}
\label{tab:multi-gpu}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Metric} & \textbf{Disk} & \textbf{RAM} & \textbf{Speedup} \\
\midrule
Checkpoint (ms) & 4673$\pm$315 & 512$\pm$3 & \textbf{9.1$\times$} \\
Recovery (ms) & 1145$\pm$71 & 248$\pm$93 & \textbf{4.6$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding 1}: Single-GPU checkpoint speedup of \textbf{13.7$\times$} aligns with the original paper's reported $>$13$\times$ faster failure recovery.

\textbf{Key Finding 2}: Recovery speedup of \textbf{6.1$\times$} is below the paper's reported $>$13$\times$. We attribute this to our scaled-down environment.

\textbf{Key Finding 3}: Training throughput actually \textit{improves} by 100\% with RAM checkpointing, as the training loop is not blocked waiting for slow disk I/O.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_speedup_summary.png}
\caption{Summary of speedup factors achieved. The 13.7$\times$ checkpoint speedup matches the paper's $>$13$\times$ target.}
\label{fig:speedups}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_checkpoint_time.png}
\caption{Checkpoint save time comparison. RAM-based checkpointing is 13.7$\times$ faster than disk-based.}
\label{fig:checkpoint}
\end{figure}

\subsection{Comparison with Original Paper}

The original Gemini paper used 16 AWS p4d.24xlarge instances (128$\times$ NVIDIA A100 GPUs) and tested with models including GPT-2, BERT, and RoBERTa scaled up to 100 billion parameters. Their reported results:
\begin{itemize}
    \item $>$13$\times$ faster failure recovery
    \item $>$100$\times$ faster checkpoint/retrieval vs. remote storage
    \item 92\% reduction in wasted training time
\end{itemize}

Our reproduction on 4 A100 GPUs achieves:
\begin{itemize}
    \item Checkpoint speedup: \textbf{13.7$\times$} (matches paper's $>$13$\times$)
    \item Recovery speedup: \textbf{6.1$\times$} (below paper's $>$13$\times$)
\end{itemize}

The recovery gap is expected due to: (1) smaller cluster size reducing the impact of parallel recovery, (2) application-level failure simulation vs. real process failures, and (3) network variability in cloud environment.

\section{Discussion and Reflection}

\subsection{What Worked Well}

\begin{itemize}[leftmargin=*]
    \item \textbf{Checkpoint speedup}: Our 13.7$\times$ speedup validates the core Gemini insight that RAM-based checkpointing dramatically reduces save latency.
    \item \textbf{Throughput improvement}: The 100\% throughput improvement demonstrates that eliminating disk I/O from the critical training path has compounding benefits.
    \item \textbf{Modular implementation}: Our separation of concerns (trainer, checkpoint manager) enabled rapid iteration and testing.
\end{itemize}

\subsection{Limitations}

\begin{itemize}[leftmargin=*]
    \item \textbf{Scale}: Our 4-GPU experiments don't capture the full benefits of Gemini at production scale (64–1024 GPUs).
    \item \textbf{Failure injection}: Simulated failures may not perfectly capture real recovery dynamics.
    \item \textbf{Memory overhead}: Each checkpoint requires $\sim$1.9 GB RAM, which may be problematic for memory-constrained environments.
\end{itemize}

\subsection{Future Work}

With more time and resources, we would explore:
\begin{itemize}[leftmargin=*]
    \item Scaling to 16–64 GPUs to better match the original paper's evaluation
    \item Implementing real failure injection via SIGKILL
    \item Testing with production models (GPT-2, BERT-Large)
    \item Exploring compression techniques to reduce checkpoint memory footprint
\end{itemize}

\subsection{Key Takeaways}

This reproduction study confirms that in-memory checkpointing is a practical and highly effective technique for accelerating failure recovery in distributed training. Even in a scaled-down cloud environment with 4 A100 GPUs, we achieve substantial speedups that validate Gemini's core contribution.

\bibliographystyle{plain}
\begin{thebibliography}{1}

\bibitem{gemini2023}
Wang et al., \textit{Gemini: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints}, SOSP 2023.
\url{https://dl.acm.org/doi/10.1145/3600006.3613145}

\end{thebibliography}

\end{document}